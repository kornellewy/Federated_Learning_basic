{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning basic",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtvuF4hCvOV2",
        "colab_type": "text"
      },
      "source": [
        "https://ai.googleblog.com/2017/04/federated-learning-collaborative.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtWpiFkmrbfh",
        "colab_type": "text"
      },
      "source": [
        "https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pW9-ssSrB2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI6jh3SJrHRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ZctGQqrgFw",
        "colab_type": "text"
      },
      "source": [
        "And we also add those specific to PySyft. In particular we define the remote workers alice and bob, which will hold the remote data while a local worker (or client) will orchestrate the learning task, as shown on the schema in Figure 1. Note that we use virtual workers: these workers behave exactly like normal remote workers except that they live in the same Python program. Hence, we still serialize the commands to be exchanged between the workers but we don't really send them over the network. This way, we avoid all network issues and can focus on the core logic of the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mf242etrLNA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4fda0570-cde3-4554-a167-c9bd6db524e7"
      },
      "source": [
        "import syft as sy\n",
        "# hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "hook = sy.TorchHook(torch)\n",
        "#NEW: define remote worker bob\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 10:32:19.819696 140541152704384 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0725 10:32:19.839627 140541152704384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URUWtsaVsEpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp8yduLwsZJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Arguments()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97FGgGNAsbAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# device\n",
        "cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "# seed\n",
        "torch.manual_seed(args.seed)\n",
        "# for data load for workes\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShBKw49Qsx3f",
        "colab_type": "text"
      },
      "source": [
        "We first load the data and transform the training Dataset into a Federated Dataset using the .federate method: it splits the dataset in two parts and send them to the workers alice and bob. This federated dataset is now given to a Federated DataLoader which will iterate over remote batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDpGgNVGtIUM",
        "colab_type": "text"
      },
      "source": [
        "# **IMPORTANT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAJBwxH3sdCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# key FederatedDataLoader \n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                   transforms.ToTensor(),\n",
        "                   transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "# we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        ".federate((bob, alice)), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "# and standart data loader\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB8us3Sps5V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# jast normal nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzf2QB_9uDP4",
        "colab_type": "text"
      },
      "source": [
        "For the train function, because the data batches are distributed across alice and bob, you need to send the model to the right location for each batch using model.send(...). Then, you perform all the operations remotely with the same syntax like you're doing local PyTorch. When you're done, you get back the model updated and the loss to look for improvement using the .get() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1uqi4Wgs7eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
        "        # send the model to the right location\n",
        "        model.send(data.location) \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # get the model back\n",
        "        model.get()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            #get the loss back\n",
        "            loss = loss.get() \n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EUyjG9puXgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test on local machine\n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
        "            pred = output.argmax(1, keepdim=True) \n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8m6sE_Vunto",
        "colab_type": "text"
      },
      "source": [
        "The training is now done as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGoFpSOAuliP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a00f9c21-b053-4f83-a9b8-eb7e2f5dd77f"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305134\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.273475\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.216174\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.156802\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.139429\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.053059\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.896587\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 1.917239\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 1.655076\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.440329\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 1.231347\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.983715\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.867023\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.890953\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.861902\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.654317\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.587091\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.693011\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.593099\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.531986\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.400264\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.455322\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.438987\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.398361\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.371324\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.289150\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.416670\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.304617\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.367789\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.386265\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.314097\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.238745\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.534601\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.369211\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.464785\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.279363\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.238020\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.182981\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.321955\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.187752\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.286318\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.365828\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.523223\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.147624\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.131381\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.224323\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.274111\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.215077\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.143916\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.291434\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.230507\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.268499\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.205015\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.147734\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.187934\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.171631\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.478283\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.303621\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.243553\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.263034\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.239960\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.456759\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.200484\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.256438\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.196973\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.270759\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.191681\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.118813\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.176458\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.174667\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.193119\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.349294\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.220412\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.191737\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.155758\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.323814\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.246998\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.254214\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.273938\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.266370\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.150609\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.129327\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.276623\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.212820\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.182833\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.143227\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.118357\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.223088\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.214281\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.085184\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.081163\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.119667\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.172809\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.143191\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.103221\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.244422\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.402155\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.105845\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.349342\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.211654\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.147023\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.192190\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.103523\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.148321\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.148263\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.060192\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.108846\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.152352\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.152677\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.111557\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.186860\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.138993\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.118514\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.221352\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.132464\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.063139\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.124831\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.163303\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.089060\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.148574\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.046008\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.156941\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.132319\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.099179\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.159745\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.054083\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.210852\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.157456\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.115076\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.144074\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.229732\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.074357\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.181132\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.197346\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.067677\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.065908\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.206655\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.115207\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.153855\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.079507\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.126364\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.038445\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.063148\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.074466\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.056466\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.158469\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.163727\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.109543\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.156446\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.072912\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.130330\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.074369\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.093282\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.090162\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.161415\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.128906\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.049100\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.074256\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.113933\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.122479\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.153609\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.138244\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.107693\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.048192\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.102147\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.116528\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.085225\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.072129\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.198065\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.101033\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.082122\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.095091\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.154688\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.106988\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.062860\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.032838\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.167880\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.069700\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.073842\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.049357\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.098377\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.113492\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.091218\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.041372\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.111734\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.083618\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.064205\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.069066\n",
            "\n",
            "Test set: Average loss: 0.0899, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.081291\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.115016\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.148862\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.080039\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.141026\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.090113\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.184774\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.286939\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.261452\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.072767\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.129465\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.095007\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.078837\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.039845\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.056576\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.206170\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.121223\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.132383\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.249777\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.165334\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.030447\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.121717\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.089990\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.171353\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.178415\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.157047\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.067125\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.150552\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.075970\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.017012\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.028152\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.043230\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.030028\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.046844\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.119822\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.148301\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.055311\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.110982\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.054740\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.034426\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.204520\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.036480\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.291176\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.095636\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.087709\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.055141\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.157873\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.095859\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.064245\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.020321\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.041318\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.025072\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.051386\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.058133\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.071311\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.096919\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.166712\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.039723\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.143135\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.028540\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.034051\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.071771\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.019140\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.082193\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.018273\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.118543\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.169696\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.055946\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.052105\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.054504\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.026263\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.080822\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.074233\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.068628\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.116148\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.024500\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.187242\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.249473\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.099400\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.078691\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.147570\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.026502\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.112970\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.049495\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.055414\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.080249\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.050256\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.067579\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.019180\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.078065\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.057820\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.055631\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.017225\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.463102\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.146829\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.140821\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.027331\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.065596\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.088037\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.096970\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.045302\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.090846\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.103507\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.101460\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.018280\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.076733\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.038305\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.030344\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.026687\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.068318\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.031968\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.078717\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.116785\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.107049\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.033536\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.105321\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.228651\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.015653\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.047861\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.065255\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.145419\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.048203\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.038587\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.045567\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.075156\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.041997\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.035451\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.083796\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.028792\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.061869\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.024896\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.112962\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.088006\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.143771\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.098854\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.158239\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.175964\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.027592\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.062299\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.093517\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.153039\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.024540\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.018805\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.144859\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.021880\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.150391\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.056411\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.085769\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.029585\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.065254\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.097678\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.009927\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.057545\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.080230\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.044069\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.025078\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.092041\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.049970\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.012687\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.133378\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.135398\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.033415\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.025962\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.057950\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.059852\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.078188\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.020599\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.044933\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.068190\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.040647\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.027091\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.131731\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.166192\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.022503\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.088655\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.067753\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.016120\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.076061\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.064018\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.149623\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.083822\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.073588\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.047142\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.056050\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.061888\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.024935\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.056401\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.017513\n",
            "\n",
            "Test set: Average loss: 0.0548, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.082590\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.191632\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.027748\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.040117\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.027606\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.013729\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.028699\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.149101\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.051579\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.035378\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.110916\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.087541\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.063442\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.069622\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.153325\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.022147\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.021511\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.017715\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.165380\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.160860\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.127810\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.046082\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.027302\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.065605\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.101586\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.028328\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.019454\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.054320\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.040218\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.057851\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.034714\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.036103\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.031931\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.097715\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.080613\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.042281\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.032395\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.022558\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.066808\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.026322\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.083409\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.151458\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.022107\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.101277\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.024551\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.017720\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.105302\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.047151\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.039595\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.053231\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.094888\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.069773\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.054388\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.052219\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.113465\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.133883\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.079842\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.037761\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.046184\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.020413\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.029371\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.138446\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.073264\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.011653\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.031342\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.354484\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.026142\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.032137\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.022094\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.017510\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.041650\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.009573\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.023011\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.040824\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.039383\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.015553\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.009420\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.040539\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.062981\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.019704\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.050176\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.039655\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.020420\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.049979\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.016948\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.032157\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.063825\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.097401\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.012206\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.016439\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.027165\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.045493\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.014956\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.061822\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.052845\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.041022\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.101479\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.101472\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.060263\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.025262\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.011245\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.032220\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.063896\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.090635\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.011883\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.048127\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.111328\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.049169\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.026860\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.014489\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.082701\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.014496\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.086852\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.037308\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.038465\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.036762\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.032877\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.034338\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.097659\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.065059\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.072403\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.053820\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.006761\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.074201\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.042853\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.034724\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.049317\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.073565\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.084093\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.114023\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.048715\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.074322\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.002777\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.007704\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.014966\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.013714\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.019984\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.044726\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.051556\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.036293\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.036134\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.022428\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.087016\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.036529\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.062339\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.010435\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.037346\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.031494\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.026880\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.054342\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.033832\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.048298\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.082996\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.027112\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.006616\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.046024\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.024264\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.076249\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.018658\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.022150\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.041769\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.061550\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.081969\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.007849\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.118383\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.005613\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.064405\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.032996\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.224602\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.051831\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.129895\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.005333\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.024883\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.005739\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.019321\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.031440\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.164235\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.017333\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.053846\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.066497\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.015297\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.024572\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.021320\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.107925\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.099601\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.069870\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.090656\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.012264\n",
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.047999\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.034098\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.104362\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.022044\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.052735\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.013907\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.012739\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.007364\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.008462\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.040412\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.051907\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.045256\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.109332\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.032891\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.011673\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.027032\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.021082\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.054524\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.075890\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.028709\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.071308\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.019882\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.004901\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.029999\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.027513\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.102506\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.080353\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.163230\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.015849\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.030652\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.091138\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.041317\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.067018\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.063134\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.235911\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.012818\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.014684\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.021896\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.013434\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.083485\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.034001\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.199663\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.021518\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.040660\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.005133\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.012901\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.009707\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.065735\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.009388\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.011764\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.010889\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.058238\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.008698\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.016098\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.024058\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.034820\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.140354\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.029954\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.008681\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.033407\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.043035\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.037521\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.177179\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.035829\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.068473\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.015905\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.012116\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.005309\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.009330\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.009162\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.056317\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.002745\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.059630\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.023628\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.088340\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.044709\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.131274\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.030067\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.070452\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.040097\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.247079\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.037277\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.030579\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.113283\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.033389\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.022107\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.044931\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.021567\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.082374\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.077737\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.176802\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.029829\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.071788\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.026903\n",
            "\n",
            "Test set: Average loss: 0.0446, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.056659\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.003103\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.008533\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.038179\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.011796\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.092766\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.075729\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.037842\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.062266\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.080880\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.019784\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.074304\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.017966\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.068654\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.008715\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.011010\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.008458\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.144776\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.032439\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.033375\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.010426\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.135810\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.059960\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.023616\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.008220\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.007761\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.002568\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.031869\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.014213\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.046544\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.024362\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.113858\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.030146\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.003008\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.030363\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.025760\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.094172\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.015257\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.073744\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.046612\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.158675\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.059743\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.002846\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.050142\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.044632\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.002485\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.022649\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.033663\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.041518\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.023581\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.194519\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.058854\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.023630\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.061561\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.016703\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.039166\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.025113\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.031350\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.027913\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.044543\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.068506\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.008879\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.012677\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.045703\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.115659\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.040274\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.046704\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.002334\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.051884\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.019379\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.146744\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.019365\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.055802\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.148652\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.007601\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.028705\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.114998\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.004698\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.014061\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.043863\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.021254\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.041628\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.063689\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.003844\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.037571\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.011502\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.019327\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.010487\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.036807\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.004992\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.081713\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.007085\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.012849\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.138579\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.005221\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.084376\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.061507\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.011011\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.094140\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.014418\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.019707\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.011745\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.013157\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.009392\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.010081\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.035735\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.062374\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.011431\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.011509\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.017745\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.014124\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.037441\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.070276\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.135452\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.026995\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.006879\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.015284\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.033568\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.021353\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.010064\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.072461\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.040855\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.097195\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.056922\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.022991\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.001831\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.011748\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.007182\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.009269\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.074973\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.071696\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.016536\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.010128\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.030944\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.034434\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.023989\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.034639\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.069216\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.026672\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.127856\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.015918\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.003149\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.044178\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.008925\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.002760\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.049925\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.097224\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.011822\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.028268\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.004451\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.029053\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.034851\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.042928\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.007711\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.010666\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.088333\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.005372\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.021169\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.024116\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.007750\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.031291\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.022429\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.037185\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.004419\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.006216\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.030606\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.049544\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.009908\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.156226\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.013750\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.026456\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.033159\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.006207\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.010933\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.014837\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.072524\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.020404\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.023534\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.019514\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.017296\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.013314\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.006574\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.074066\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.084104\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.010870\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.006524\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.039178\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.006324\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 0/10000 (0%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.004203\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.009176\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.011273\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.030270\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.004841\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.009510\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.029526\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.009872\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.020855\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.002077\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.056733\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.006966\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.088129\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.049797\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.031724\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.031981\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.037559\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.004699\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.037131\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.007529\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.002339\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.009185\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.018396\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.029989\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.002562\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.057425\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.103454\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.035339\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.014449\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.017456\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.146149\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.022699\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.006262\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.034385\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.018905\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.003387\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.189797\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.018028\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.015050\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.009782\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.023960\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.011002\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.012849\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.007212\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.026730\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.008088\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.011871\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.081546\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.013652\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.013216\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.022778\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.027806\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.096831\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.010408\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.033577\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.048404\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.005352\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.017015\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.009835\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.112933\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.028350\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.041417\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.030024\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.078473\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.084505\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.057242\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.014510\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.037008\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.016122\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.004908\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.068341\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.015999\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.004810\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.172246\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.007947\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.076611\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.010773\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.071406\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.043203\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.191546\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.008314\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.004271\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.009764\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.018046\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.100702\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.061744\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.051636\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.003043\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.026447\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.033198\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.013565\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.048527\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.107387\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.007521\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 0/10000 (0%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlz_88l7uzY2",
        "colab_type": "text"
      },
      "source": [
        "As you observe, we only had to modify 10 lines of code to upgrade the official Pytorch example on MNIST to a real Federated Learning task!\n",
        "\n",
        "Of course, there are dozen of improvements we could think of. We would like the computation to operate in parallel on the workers and perform federated averaging, to update the central model every n batches only, to reduce the number of messages we use to communicate between workers while orchestrating the training, etc. These are features we're working on to make Federated Learning ready for a production environment and we'll write about them as soon as they are released!\n",
        "\n",
        "You should now be able to do Federated Learning by yourself! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvAzZg0nupGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}